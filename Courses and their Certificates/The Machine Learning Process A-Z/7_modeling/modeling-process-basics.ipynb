{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-09T02:49:45.346345Z","iopub.execute_input":"2022-11-09T02:49:45.347434Z","iopub.status.idle":"2022-11-09T02:49:45.380195Z","shell.execute_reply.started":"2022-11-09T02:49:45.347327Z","shell.execute_reply":"2022-11-09T02:49:45.379378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The ML Modeling Process Basics \nIn this notebook, we will go through some of the basic techinques for modeling data. This is a companion workbook for the 365 Data Science course on ML Process. This notebook only foucses on implementation. Check out the course or the documentation for the in-depth explanations of each step\n\nIn this case, we will be trying to predict if we can predict a stroke from the above dataset. \n\nWe will cover:\n- Baseline creation\n- Model selection\n- Parameter tuning\n     - manual\n     - gridsearch\n     - random search\n     - basian optomization\n- Ensemble models\n\nimblearn Documentation: https://imbalanced-learn.org/stable/\n\n### On the Data \nThis dataset is a good representation of real world data that can have valuable impact when analyzed. We will be exploring the accuracy of different models for predicting if someone will have a stroke or not. We will first lightly explore the data, create our train, test / validation sets, then we will ceate a baseline model. To get the best results we will compare other algorithms to our basline and use various parameter tuning techniques to see which model produces the best results. At the end we will explore some ensemble models to see what produces the best results. \n\nThe focus of this notebook is the modeling process. If you're interested in the specifics of differen machine learning algorithms, check out our other course specifically on that. ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:49:45.643643Z","iopub.execute_input":"2022-11-09T02:49:45.644087Z","iopub.status.idle":"2022-11-09T02:49:45.678644Z","shell.execute_reply.started":"2022-11-09T02:49:45.644050Z","shell.execute_reply":"2022-11-09T02:49:45.677700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore the data \nWe will take a very high level overview of the data in this section. We mostly want to see if there are any null values. For full workbooks on exploratory data analysis and on dealing with null values, check out these notebooks:\n- EDA:\n- Dealing with Null Values: \n- Dealing with Imbalanced Data: https://www.kaggle.com/kenjee/dealing-with-imbalanced-data","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:49:45.983833Z","iopub.execute_input":"2022-11-09T02:49:45.984324Z","iopub.status.idle":"2022-11-09T02:49:45.994436Z","shell.execute_reply.started":"2022-11-09T02:49:45.984287Z","shell.execute_reply":"2022-11-09T02:49:45.992715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at basic data for continuous variables \ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:49:46.148090Z","iopub.execute_input":"2022-11-09T02:49:46.148502Z","iopub.status.idle":"2022-11-09T02:49:46.209530Z","shell.execute_reply.started":"2022-11-09T02:49:46.148471Z","shell.execute_reply":"2022-11-09T02:49:46.208452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include=np.object)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:49:46.305202Z","iopub.execute_input":"2022-11-09T02:49:46.305638Z","iopub.status.idle":"2022-11-09T02:49:46.332990Z","shell.execute_reply.started":"2022-11-09T02:49:46.305600Z","shell.execute_reply":"2022-11-09T02:49:46.331628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#small enough number of null values we will just remove them.\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:49:47.150019Z","iopub.execute_input":"2022-11-09T02:49:47.150393Z","iopub.status.idle":"2022-11-09T02:49:47.161613Z","shell.execute_reply.started":"2022-11-09T02:49:47.150364Z","shell.execute_reply":"2022-11-09T02:49:47.160436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for possible nulls in categoricals / non answers \nfor i in df.select_dtypes(include=['object']).columns:\n    print(df[i].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:49:47.324793Z","iopub.execute_input":"2022-11-09T02:49:47.325207Z","iopub.status.idle":"2022-11-09T02:49:47.339766Z","shell.execute_reply.started":"2022-11-09T02:49:47.325174Z","shell.execute_reply":"2022-11-09T02:49:47.338490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop null values and create dummy variables \n#scale data? \ndf_cleaned = df.dropna()\ndf_cleaned = df_cleaned.drop('id',axis=1).reset_index(drop=True)\ndf_final = pd.get_dummies(df_cleaned)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T03:26:14.156724Z","iopub.execute_input":"2022-11-09T03:26:14.157192Z","iopub.status.idle":"2022-11-09T03:26:14.179943Z","shell.execute_reply.started":"2022-11-09T03:26:14.157151Z","shell.execute_reply":"2022-11-09T03:26:14.178887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.columns","metadata":{"execution":{"iopub.status.busy":"2022-11-09T03:26:15.759282Z","iopub.execute_input":"2022-11-09T03:26:15.759704Z","iopub.status.idle":"2022-11-09T03:26:15.767407Z","shell.execute_reply.started":"2022-11-09T03:26:15.759671Z","shell.execute_reply":"2022-11-09T03:26:15.766051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create train test split \n\nfrom sklearn.model_selection import train_test_split\nX = df_final.drop('stroke', axis =1)\ny = df_final.loc[:,['stroke']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T03:36:17.020280Z","iopub.execute_input":"2022-11-09T03:36:17.020689Z","iopub.status.idle":"2022-11-09T03:36:17.034066Z","shell.execute_reply.started":"2022-11-09T03:36:17.020658Z","shell.execute_reply":"2022-11-09T03:36:17.032748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#balance the data (SMOTE)\nfrom imblearn.over_sampling import SMOTE \nsmote = SMOTE(sampling_strategy =1)\n\nX_train, y_train = smote.fit_resample(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T03:36:17.905849Z","iopub.execute_input":"2022-11-09T03:36:17.907128Z","iopub.status.idle":"2022-11-09T03:36:17.941015Z","shell.execute_reply.started":"2022-11-09T03:36:17.907085Z","shell.execute_reply":"2022-11-09T03:36:17.939078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a Basline Model\nHow can we tell if our machine learning models are any good? To evaluate performance, we need to benchmark against something. In this case, we will create two baslines for our model. First, we can simply look at the average of our data for a numeric value. If we were going to predict the age, we could simply guess the average age for every candidate. \n\nOn the other hand, for a categorical variable, we could simply guess 50/50 or the ratio of the categories in the data. In this case, the stroke data is imbalanced with 3160 / 3289 samples being of the non-stroke cateogry. That means that if we guessed that everyone in the sample didn't have a stroke, we would have a 96.1% success rate. Since this data is slightly imblanaced, this would not be a good baseline for our model.\n\nOne of the most important steps that we need to take is choosing a good evluation metric. The notebook that covers specific evaluation metrics can be located here: \n\nAccuracy does not make sense because of the imbalanced nature of the data. For this example we will use F1 score as our model evaluation metric.\n\n- F1 is calculated by 2*((precision*recall)/(precision+recall))\n\n- Instead of a simple accuracy calculation which would give us a baseline of 96.1%, F1 score gives us an undefined number since both the precision and recall of a model that only predicted negatives would equal 0. \n\n- In this case, we want to use a simple basleline model like Naive Bayes to set our baseline based off of f1 score. You can use most models to create a baseline, but I like Naive bayes because it is quick and doesn't require much parameter tuning. (Full breakdown of Naieve Bayes in or Algorithms Course)\n","metadata":{}},{"cell_type":"code","source":"#import cross validation score\nfrom sklearn.model_selection import cross_val_score\n\n#import Naive Bayes Classifier \nfrom sklearn.naive_bayes import GaussianNB\n\n#create classifier object\nnb = GaussianNB()\n\n#run cv for NB classifier\nfrom sklearn.metrics import classification_report\n\nnb_accuracy = cross_val_score(nb,X_train,y_train.values.ravel(), cv=5, scoring ='accuracy')\nnb_f1 = cross_val_score(nb,X_train,y_train.values.ravel(), cv=5, scoring ='f1')\n\nprint('nb_accuracy: ' +str(nb_accuracy))\nprint('nb F1_Macro Score: '+str(nb_f1))\nprint('nb_accuracy_avg: ' + str(nb_accuracy.mean()) +'  |  lr_f1_avg: '+str(nb_f1.mean()))\n\n\n#With these F1 scores, we can begin evaluating our model. While the accuracy is lower than if we only predicted 0 every time,\n# our f1 score suggests we are doing a far better job of predicting stroke outcomes. ","metadata":{"execution":{"iopub.status.busy":"2022-11-09T03:36:19.997792Z","iopub.execute_input":"2022-11-09T03:36:19.999008Z","iopub.status.idle":"2022-11-09T03:36:20.101260Z","shell.execute_reply.started":"2022-11-09T03:36:19.998934Z","shell.execute_reply":"2022-11-09T03:36:20.100045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Comparison & Selection \nAfter we have a baseline model to compare against, we want to evaluate how other models might perform on the same data. I like to experiment with other basic models with very little paramater tuning to see what performs well. This isn't an exact science and many people may do this step differently. After we set up the models, we can begin experimenting with parameter tuning. I find that model selection and parameter tuning is often an iterative process. For an analysis like this, trying different models, changing parameters, and experimenting with new engineered features is where I find myself spending most of my time working. \n\nIn this section we will try:\n- Logistic regression\n- Decision Tree\n- Support Vector Machines (SVM)\n- K Nearest Neighbors (KNN)","metadata":{}},{"cell_type":"code","source":"#Let's now experiment with a few different basic models \n\n## Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=32)\n\nlr_accuracy = cross_val_score(lr,X_train,y_train.values.ravel(), cv=5, scoring ='accuracy')\nlr_f1 = cross_val_score(lr,X_train,y_train.values.ravel(), cv=5, scoring ='f1')\n\nprint('lr_accuracy: ' +str(lr_accuracy))\nprint('lr F1_Macro Score: '+str(lr_f1))\nprint('lr_accuracy_avg: ' + str(lr_accuracy.mean()) +'  |  lr_f1_avg: '+str(lr_f1.mean())+'\\n')\n\n## Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\n\ndt_accuracy = cross_val_score(dt,X_train,y_train.values.ravel(), cv=5, scoring ='accuracy')\ndt_f1 = cross_val_score(dt,X_train,y_train.values.ravel(), cv=5, scoring ='f1')\n\nprint('dt_accuracy: ' +str(dt_accuracy))\nprint('dt F1_Macro Score: '+str(dt_f1))\nprint('dt_accuracy_avg: ' + str(dt_accuracy.mean()) +'  |  dt_f1_avg: '+str(dt_f1.mean())+'\\n')\n\n## SVM - Requires feature scaling (more on features scaling in this notebook: )\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline, Pipeline #creates chained events \n\nsvc = Pipeline([('scale',StandardScaler()), ('svc',SVC())])\n\nsvc_accuracy = cross_val_score(svc,X_train,y_train.values.ravel(), cv=5, scoring ='accuracy')\nsvc_f1 = cross_val_score(svc,X_train,y_train.values.ravel(), cv=5, scoring ='f1')\n\nprint('svc_accuracy: ' +str(svc_accuracy))\nprint('svc F1_Macro Score: '+str(svc_f1))\nprint('svc_accuracy_avg: ' + str(svc_accuracy.mean()) +'  |  svc_f1_avg: '+str(svc_f1.mean()) +'\\n')\n\n\n## KNN \nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nknn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\nknn_accuracy = cross_val_score(knn,X_train,y_train.values.ravel(), cv=5, scoring ='accuracy')\nknn_f1 = cross_val_score(knn,X_train,y_train.values.ravel(), cv=5, scoring ='f1')\n\nprint('knn_accuracy: ' +str(knn_accuracy))\nprint('knn F1_Macro Score: '+str(knn_f1))\nprint('knn_accuracy_avg: ' + str(knn_accuracy.mean()) +'  |  knn_f1_avg: '+str(knn_f1.mean()))\n","metadata":{"execution":{"iopub.status.busy":"2022-11-09T03:36:20.783732Z","iopub.execute_input":"2022-11-09T03:36:20.785006Z","iopub.status.idle":"2022-11-09T03:36:27.159013Z","shell.execute_reply.started":"2022-11-09T03:36:20.784927Z","shell.execute_reply":"2022-11-09T03:36:27.157819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Comparison \nIt looks like we chose a pretty good baseline. While it slightly underperforms all of our new models in accuracy, it outperforms all of them in F1 score which is what we care about most for this analysis. Let's look at how everything stacks up. \n\n|Model          | F1 Score      |\n| :------------ | :-----------: |\n| **Baseline Naive Bayes**  | **81.0%**     |\n| K Nearest Neighbors | **97.2%**     |\n| Support Vector Clf | **95.8%**     |\n| Decision Tree  | **95.1%**     |\n| Logistic Regression  | **94.0%**     |\n\n\nWhile all of our models outperformed our basline, we still can do better. We can now parameter tune! That means that we make adjustments to the model parameter inputs to better compensate for our specific data. One of the drawbacks of Naive Bayes is that it has virtually no paramaters that we can tune, so our inital results are about the best we will get with it without making changes to our data. \n\n","metadata":{}},{"cell_type":"markdown","source":"## Manual Feature Selection\nLet's try to do some parameter tuning with a few of these models:\n\nLet's start with K Nearest Neighbors,which has a few parameters we can adjust, one of them being the number of K. K is how many other datapoints it uses to make its classification. If k= 3 it uses it sees what the samples 3 closest neighbors is and classifies it as the most common one. If k = 5, it uses its 5 closest datapoints. Let's change the number of k and see if that changes our results. ","metadata":{}},{"cell_type":"code","source":"#Knn Model Comparison \n\n#here we will loop through and see which value of k performs the best. \n\nfor i in range(1,20):\n    knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=i))\n    knn_f1 = cross_val_score(knn,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n    print('K ='+(str(i)) + (': ') + str(knn_f1.mean()))\n\n#What we find is that k=2 is the best estimator for this specific model.\n#We also realize that KNN may not be the best approach here because of the imbalanced data. \n#The larger the K is, the more of the majority class will automatically be included.","metadata":{"execution":{"iopub.status.busy":"2022-11-09T03:36:41.997547Z","iopub.execute_input":"2022-11-09T03:36:41.997992Z","iopub.status.idle":"2022-11-09T03:36:56.342034Z","shell.execute_reply.started":"2022-11-09T03:36:41.997937Z","shell.execute_reply":"2022-11-09T03:36:56.340614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Randomized Feature Selection\nSince KNN may not be the best choice, let's explore the deicision tree. Decision trees have a lot more features we can tune. We can tweak the following:\n- criterion {gini, entropy, log loss}\n- splitter {best, random}\n- max depth {int, None}\n- min_samples_split {int, None}\n- min_samples_leave {int, None}\n- min_weight_fraction_leaf {float}\n- max_features {int, auto, sqrt, log2, None}\n- max_leaf_nodes {int, None}\n- min_impurity_decrease {float}\n- class_weight {dict, balanced, None}\n- ccp_alpha {float}\n\nThere are a lot of parameters to tune! If there are just 2 options for each one that would be 2^11, which is 2048 total configurations. In theory, there are infinate numbers of paramater configurations. How do we even get close to finding the best one? \n\nThe answer here is randomized search. We through in all the parameters that we are interested in searching, and the model will randomly select a subset and return the one that produces the best results. \n\nStill, let's manually select a few paramaters we want to evaluate on and then use randomized search:\n- criterion\n- split strategy\n- max depth\n- min_samples_split\n- max features","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\ndt = DecisionTreeClassifier(random_state = 42)\n\nfeatures = {'criterion': ['gini','entropy'],\n            'splitter': ['best','random'],\n           'max_depth': [2,5,10,20,40,None],\n           'min_samples_split': [2,5,10,15],\n           'max_features': ['auto','sqrt','log2',None]}\n\nrs_dt = RandomizedSearchCV(estimator = dt, param_distributions =features, n_iter =100, cv = 3, random_state = 42, scoring ='f1')\n\nrs_dt.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T03:36:58.287690Z","iopub.execute_input":"2022-11-09T03:36:58.288129Z","iopub.status.idle":"2022-11-09T03:37:02.102547Z","shell.execute_reply.started":"2022-11-09T03:36:58.288091Z","shell.execute_reply":"2022-11-09T03:37:02.101359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('best stcore = ' + str(rs_dt.best_score_))\nprint('best params = ' + str(rs_dt.best_params_))","metadata":{"execution":{"iopub.status.busy":"2022-11-09T03:37:02.105162Z","iopub.execute_input":"2022-11-09T03:37:02.105509Z","iopub.status.idle":"2022-11-09T03:37:02.112496Z","shell.execute_reply.started":"2022-11-09T03:37:02.105478Z","shell.execute_reply":"2022-11-09T03:37:02.111151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GridsearchCV\nWith this we have improved our model f1 score from **95.1% to 96.0%**. This is a decent increase! We also narrowed down some of the features that produced good results. We may want to try a more exhaustive search this time. Gridsearch goes through all of the possible combinations within an range and returns the best outcome. \n\nThis time, let's do an exhaustive search of a smaller number of features and see if we can improve our results even more. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n\nfeatures_gs = {'criterion': ['entropy'],\n            'splitter': ['random'],\n           'max_depth': np.arange(30,50,1), #getting more precise within range\n           'min_samples_split': [2,3,4,5,6,7,8,9],\n           'max_features': [None]}\n\ngs_dt = GridSearchCV(estimator = dt, param_grid =features_gs, cv = 3, scoring ='f1') #we don't need random state because there isn't randomization like before\n\ngs_dt.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-11-09T03:37:02.114297Z","iopub.execute_input":"2022-11-09T03:37:02.114626Z","iopub.status.idle":"2022-11-09T03:37:08.917915Z","shell.execute_reply.started":"2022-11-09T03:37:02.114596Z","shell.execute_reply":"2022-11-09T03:37:08.917059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('best stcore = ' + str(gs_dt.best_score_))\nprint('best params = ' + str(gs_dt.best_params_))\n\n#looks like we can  do a little better with this gridsearch! ","metadata":{"execution":{"iopub.status.busy":"2022-11-09T03:37:08.919998Z","iopub.execute_input":"2022-11-09T03:37:08.920353Z","iopub.status.idle":"2022-11-09T03:37:08.926604Z","shell.execute_reply.started":"2022-11-09T03:37:08.920320Z","shell.execute_reply":"2022-11-09T03:37:08.925271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bayesian Optimization\nI wonnder if we can do better than the funnel approach that we took with random search and gridsearch. What if we used a slightly smarter algorithm to help evaluate our features. Maybe we could explore all of the variables from the previous examples and see if our model missed something. This is where Bayesian Optimization comes in. This is an iterative process where our model improves its understandings of the feature inputs as it goes. (Full breakdown in the video portion of the course)\n\nNow let's try to use this with a larger feature set on the SVC classifier. This won't guarantee a better result as it still is not an exahustive search, but in theory it let's us cover ground in a more efficient way. ","metadata":{}},{"cell_type":"code","source":"from skopt import BayesSearchCV\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:53:12.704028Z","iopub.execute_input":"2022-11-09T02:53:12.704772Z","iopub.status.idle":"2022-11-09T02:53:12.784542Z","shell.execute_reply.started":"2022-11-09T02:53:12.704735Z","shell.execute_reply":"2022-11-09T02:53:12.783384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Scale data first \nsvc_est = SVC()\n\n# Choose cross validation method \ncv = StratifiedKFold(n_splits = 3)\n\n#scale data \nbscaler = StandardScaler().fit(X_train)\nx_train_scaled = bscaler.transform(X_train)\n\nfrom skopt import BayesSearchCV\n\nfeatures_svc = {\n        'C': (1e-3, 1e+3, 'log-uniform'), #svc__ lets us choose the palce to pass parameter in pipeline above\n        'gamma': (1e-3, 1e+1, 'log-uniform'),\n        'kernel': ['linear', 'poly', 'rbf'],  # categorical parameter\n        'degree': (1, 8),  # integer valued parameter\n    }\n\nbs_svc = BayesSearchCV(svc_est,features_svc, random_state=42, n_iter= 10, cv= cv, scoring ='f1')\n \nbs_svc.fit(x_train_scaled,y_train.values.ravel())","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:53:14.663528Z","iopub.execute_input":"2022-11-09T02:53:14.663928Z","iopub.status.idle":"2022-11-09T02:53:37.239416Z","shell.execute_reply.started":"2022-11-09T02:53:14.663897Z","shell.execute_reply":"2022-11-09T02:53:37.238190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('best stcore = ' + str(bs_svc.best_score_))\nprint('best params = ' + str(bs_svc.best_params_))\n\n#while this didn't outperform our gridsearch, it is still a good approach to try when dealing with many different feature options. \n#it still did outperform our originial random search. 95.8 to 96.4%!","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:53:56.946397Z","iopub.execute_input":"2022-11-09T02:53:56.946798Z","iopub.status.idle":"2022-11-09T02:53:56.953062Z","shell.execute_reply.started":"2022-11-09T02:53:56.946767Z","shell.execute_reply":"2022-11-09T02:53:56.952044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Selecting a Model\nWe still haven't been able to do better than our baseline. In most cases, we to tune multiple different models until we reach one that performs the best based on our evaluation criteria. We also want to use other considerations like training time, prediction time, prediction time or interperetability to select selct the best model for our use case. \n\nSince we have one tuned model, lets see if we can improve it by combining it with a few of the other models we have used. This process is called ensembling. In the case of classification, we often use a popular vote metric to select the best model. \n\nLet's see if an ensemble model of these three classifiers outperforms our baseline model. ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\ndt_voting = DecisionTreeClassifier(**{'criterion': 'entropy', 'max_depth': 30, 'max_features': None, 'min_samples_split': 6, 'splitter': 'random', 'random_state' : 42}) # ** allows you to pass in parameters as dict\nknn_voting = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=2))\nsvc_voting = make_pipeline(StandardScaler(), SVC(**{'C':1.8214548318355843,'gamma':0.09679809588492402,'kernel':'rbf'}))\n\nens = VotingClassifier(estimators = [('dt', dt_voting), ('knn', knn_voting), ('svc',svc_voting)], voting = 'hard')\n","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:54:33.125208Z","iopub.execute_input":"2022-11-09T02:54:33.125627Z","iopub.status.idle":"2022-11-09T02:54:33.133809Z","shell.execute_reply.started":"2022-11-09T02:54:33.125595Z","shell.execute_reply":"2022-11-09T02:54:33.132491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_accuracy = cross_val_score(ens,X_train,y_train.values.ravel(), cv=5, scoring ='accuracy')\nvoting_f1 = cross_val_score(ens,X_train,y_train.values.ravel(), cv=5, scoring ='f1')\n\nprint('voting_accuracy: ' +str(voting_accuracy))\nprint('voting F1_Macro Score: '+str(voting_f1))\nprint('voting_accuracy_avg: ' + str(voting_accuracy.mean()) +'  |  voting_f1_avg: '+str(voting_f1.mean()))","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:54:35.219550Z","iopub.execute_input":"2022-11-09T02:54:35.220249Z","iopub.status.idle":"2022-11-09T02:54:40.144706Z","shell.execute_reply.started":"2022-11-09T02:54:35.220209Z","shell.execute_reply":"2022-11-09T02:54:40.142611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacked classifier \nIn the case of the voting classifer, we didn't get better performance than our baseline model. Let's now try another type of ensembling called stacking. With stacking, we use the outputs of each of our individual models as features into a new model. In this case, where we have a decision tree, a naive baayes classifier, and a svc classifier, these will be the three features that a new model predicts on. \n\nLet's try running these three through a Naive Bayes Classifier and see what the results look like. ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n\nens_stack = StackingClassifier(estimators = [('dt', dt_voting), ('knn', knn_voting), ('svc',svc_voting)], final_estimator = LogisticRegression())","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:54:51.660147Z","iopub.execute_input":"2022-11-09T02:54:51.660553Z","iopub.status.idle":"2022-11-09T02:54:51.667345Z","shell.execute_reply.started":"2022-11-09T02:54:51.660521Z","shell.execute_reply":"2022-11-09T02:54:51.666082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stack_accuracy = cross_val_score(ens_stack,X_train,y_train.values.ravel(), cv=5, scoring ='accuracy')\nstack_f1 = cross_val_score(ens_stack,X_train,y_train.values.ravel(), cv=5, scoring ='f1')\n\nprint('stacking_accuracy: ' +str(stack_accuracy))\nprint('stacking F1_Macro Score: '+str(stack_f1))\nprint('stacking_accuracy_avg: ' + str(stack_accuracy.mean()) +'  |  stack_f1_avg: '+str(stack_f1.mean()))\n\n#in this case it didn't outperfrom, but it often does.","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:54:57.843667Z","iopub.execute_input":"2022-11-09T02:54:57.844235Z","iopub.status.idle":"2022-11-09T02:55:18.926414Z","shell.execute_reply.started":"2022-11-09T02:54:57.844180Z","shell.execute_reply":"2022-11-09T02:55:18.925409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble Models\nThe last main type of ensemble approach that we see is one that is designed that way algorithmically. Typically, random forest or gradient boosted models have ensembling built into their implementation. Let's explor random forest and see how this approach works for our data. (We have a breakdown of the main ensembling techniques in our full course on algorithms). These algorithms leverage multiple decision trees to either vote or give pass information on to subsequent models. ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n#first let's try a non-tuned implementation \nrf = RandomForestClassifier()\n\nrf_accuracy = cross_val_score(rf,X_train,y_train.values.ravel(), cv=5, scoring ='accuracy')\nrf_f1 = cross_val_score(rf,X_train,y_train.values.ravel(), cv=5, scoring ='f1')","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:55:24.299249Z","iopub.execute_input":"2022-11-09T02:55:24.300289Z","iopub.status.idle":"2022-11-09T02:55:29.259903Z","shell.execute_reply.started":"2022-11-09T02:55:24.300249Z","shell.execute_reply":"2022-11-09T02:55:29.258689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('rf_accuracy: ' +str(rf_accuracy))\nprint('rf F1_Macro Score: '+str(rf_f1))\nprint('rf_accuracy_avg: ' + str(rf_accuracy.mean()) +'  |  rf_f1_avg: '+str(rf_f1.mean()))\n\n#of course, you can tune this model like the others! ","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:55:29.262018Z","iopub.execute_input":"2022-11-09T02:55:29.262411Z","iopub.status.idle":"2022-11-09T02:55:29.269704Z","shell.execute_reply.started":"2022-11-09T02:55:29.262375Z","shell.execute_reply":"2022-11-09T02:55:29.268552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nnb.fit(X_train,y_train.values.ravel())\nens.fit(X_train,y_train.values.ravel())\ndt_voting.fit(X_train,y_train.values.ravel())\nens_stack.fit(X_train,y_train.values.ravel())\nrf_est = RandomForestClassifier()\nrf_est.fit(X_train,y_train.values.ravel())\n\nnb_pred = nb.predict(X_test)\nens_pred = ens.predict(X_test)\ndt_pred = dt_voting.predict(X_test)\nens_stack_pred = ens_stack.predict(X_test)\nrf_pred = rf_est.predict(X_test)\n\nprint(f1_score(y_test,nb_pred))\nprint(f1_score(y_test,ens_pred))\nprint(f1_score(y_test,dt_pred))\nprint(f1_score(y_test,ens_stack_pred))\nprint(f1_score(y_test,rf_pred))","metadata":{"execution":{"iopub.status.busy":"2022-11-09T02:55:33.615275Z","iopub.execute_input":"2022-11-09T02:55:33.615676Z","iopub.status.idle":"2022-11-09T02:55:37.950087Z","shell.execute_reply.started":"2022-11-09T02:55:33.615644Z","shell.execute_reply":"2022-11-09T02:55:37.948861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve, precision_recall_curve, auc, confusion_matrix, roc_auc_score\n\ncf_matrix = confusion_matrix(y_test, rf_pred)\nplt.title('Confusion Matrix:')\nsns.heatmap(cf_matrix, annot = True, fmt = 'g', cmap = sns.cubehelix_palette(as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2022-11-09T03:00:16.621832Z","iopub.execute_input":"2022-11-09T03:00:16.622700Z","iopub.status.idle":"2022-11-09T03:00:16.838138Z","shell.execute_reply.started":"2022-11-09T03:00:16.622663Z","shell.execute_reply":"2022-11-09T03:00:16.837163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make stacking classifier \n# make random forest classifier \n# record video \n# test on test set ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-11-07T20:27:15.191182Z","iopub.status.idle":"2022-11-07T20:27:15.191598Z","shell.execute_reply.started":"2022-11-07T20:27:15.191401Z","shell.execute_reply":"2022-11-07T20:27:15.191419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n## Ensemble \n## Model Evaluation \n","metadata":{"execution":{"iopub.status.busy":"2022-11-07T20:27:15.193678Z","iopub.status.idle":"2022-11-07T20:27:15.194336Z","shell.execute_reply.started":"2022-11-07T20:27:15.194015Z","shell.execute_reply":"2022-11-07T20:27:15.194043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}